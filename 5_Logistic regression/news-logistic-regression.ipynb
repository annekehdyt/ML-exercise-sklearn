{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(groups_train, groups_test):\n",
    "    y_train = np.array(groups_train.target)\n",
    "    y_test = np.array(groups_test.target)\n",
    "    \n",
    "    tf_vectorizer = CountVectorizer(max_df=1.0, min_df=5, binary=True)\n",
    "    \n",
    "    print('Data Vectorizer Transform start')\n",
    "    print()\n",
    "    X_train = tf_vectorizer.fit_transform(groups_train.data)\n",
    "\n",
    "    print('Train Data Transformed')\n",
    "    print('Train Data size ', X_train.shape)\n",
    "    print()\n",
    "    X_test = tf_vectorizer.transform(groups_test.data)\n",
    "    print('Test Data Transformed')\n",
    "    print('Test Data size ', X_test.shape)\n",
    "    \n",
    "    f_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, f_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_features(clf_L1, clf_L2, f_names, msg, iter_range=10):\n",
    "    idx_L2 = np.argsort(np.absolute(clf_L2.coef_)[0,:])[::-1]\n",
    "    idx_L1 = np.argsort(np.absolute(clf_L1.coef_)[0,:])[::-1]\n",
    "    \n",
    "    ### Print on Pretty Table\n",
    "    table_features = PrettyTable(['Rank', 'L2 Features', 'L2 Weight', 'L1 Features', 'L1 Weight'])\n",
    "    f_list_1 = np.zeros(iter_range, dtype='int16')\n",
    "    f_list_2 = np.ones(iter_range, dtype='int16')\n",
    "    \n",
    "    for idx in range(0,iter_range):\n",
    "        table_features.add_row([idx+1, \n",
    "                                f_names[idx_L2[idx]], \n",
    "                                np.around(clf_L2.coef_[0,idx_L2[idx]], decimals=4), \n",
    "                                f_names[idx_L1[idx]], \n",
    "                                np.around(clf_L1.coef_[0,idx_L1[idx]], decimals=4)])\n",
    "        f_list_2[idx] = idx_L2[idx]\n",
    "        f_list_1[idx] = idx_L1[idx]\n",
    "        \n",
    "    print('L2 and L1-regularized Logistic Regression Classifier', msg)\n",
    "    print('Top 10 features and weights (with absolute value)')\n",
    "    print()\n",
    "    print(table_features)\n",
    "    print(' ')\n",
    "    print('List of features in both L1 and L2 penalty :')\n",
    "    num=1\n",
    "    for i in range(0,iter_range):\n",
    "        for j in range(0,iter_range):\n",
    "            if f_list_1[i] == f_list_2[j]:\n",
    "                print('\\t', num, f_names[f_list_1[i]])\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_categories = ['comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=news_categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=news_categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Vectorizer Transform start\n",
      "\n",
      "Train Data Transformed\n",
      "Train Data size  (1181, 4933)\n",
      "\n",
      "Test Data Transformed\n",
      "Test Data size  (786, 4933)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, f_names = binarize(newsgroups_train, newsgroups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model to the Data without Z-score scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "clf_noz_l2 = LogisticRegression()\n",
    "clf_noz_l2.fit(X_train, y_train)\n",
    "print(clf_noz_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_noz_l1 = LogisticRegression(penalty='l1')\n",
    "clf_noz_l1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 and L1-regularized Logistic Regression Classifier without Z-score scaling\n",
      "Top 10 features and weights (with absolute value)\n",
      "\n",
      "+------+-------------+-----------+-------------+-----------+\n",
      "| Rank | L2 Features | L2 Weight | L1 Features | L1 Weight |\n",
      "+------+-------------+-----------+-------------+-----------+\n",
      "|  1   |   windows   |  -2.8115  |   windows   |  -3.4064  |\n",
      "|  2   |     file    |  -1.0976  |    latest   |   -2.567  |\n",
      "|  3   |    files    |  -0.9501  |     win3    |  -1.7427  |\n",
      "|  4   |     ibm     |   0.8911  |     wish    |    1.69   |\n",
      "|  5   |    window   |  -0.8903  |     nasa    |   1.6795  |\n",
      "|  6   |     win     |  -0.8678  |    window   |  -1.6084  |\n",
      "|  7   |   monitor   |   0.8255  |    floppy   |   1.5967  |\n",
      "|  8   |     win3    |   -0.819  |     ide     |   1.5538  |\n",
      "|  9   |  microsoft  |  -0.7222  |  currently  |   1.5456  |\n",
      "|  10  |   gateway   |   0.7025  |     ibm     |   1.5095  |\n",
      "|  11  |     ide     |   0.6961  |   download  |  -1.4465  |\n",
      "|  12  | motherboard |   0.6844  |      mq     |  -1.4264  |\n",
      "|  13  |     cica    |   -0.654  |    board    |   1.3872  |\n",
      "|  14  |    drive    |   0.6507  |     cica    |   -1.382  |\n",
      "|  15  |    board    |   0.6343  |    files    |   -1.37   |\n",
      "|  16  |    latest   |   -0.632  | motherboard |   1.364   |\n",
      "|  17  |     bus     |   0.5974  |   monitors  |   1.3216  |\n",
      "|  18  |  currently  |   0.5889  |  microsoft  |  -1.3186  |\n",
      "|  19  |    floppy   |   0.5752  |    moore    |  -1.2611  |\n",
      "|  20  |   download  |  -0.5737  |    thomas   |  -1.2606  |\n",
      "+------+-------------+-----------+-------------+-----------+\n",
      " \n",
      "List of features in both L1 and L2 penalty :\n",
      "\t 1 windows\n",
      "\t 2 latest\n",
      "\t 3 win3\n",
      "\t 4 window\n",
      "\t 5 floppy\n",
      "\t 6 ide\n",
      "\t 7 currently\n",
      "\t 8 ibm\n",
      "\t 9 download\n",
      "\t 10 board\n",
      "\t 11 cica\n",
      "\t 12 files\n",
      "\t 13 motherboard\n",
      "\t 14 microsoft\n"
     ]
    }
   ],
   "source": [
    "print_features(clf_noz_l1, clf_noz_l2, f_names, 'without Z-score scaling', iter_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 : 0.870229007634\n",
      "L1 : 0.839694656489\n"
     ]
    }
   ],
   "source": [
    "y_noz_l1 = clf_noz_l1.predict(X_test)\n",
    "y_noz_l2 = clf_noz_l2.predict(X_test)\n",
    "\n",
    "print('L2 :', accuracy_score(y_noz_l2, y_test))\n",
    "print('L1 :', accuracy_score(y_noz_l1, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model to the Data with Z-score scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform the X_train to dense\n",
    "X_train_dense = X_train.todense()\n",
    "X_test_dense = X_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1181, 4933)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_dense)\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(786, 4933)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test_dense)\n",
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l2 = LogisticRegression()\n",
    "clf_l2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l1 = LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 and L1-regularized Logistic Regression Classifier with Z-score scaling\n",
      "Top 10 features and weights (with absolute value)\n",
      "\n",
      "+------+---------------+-----------+-------------+-----------+\n",
      "| Rank |  L2 Features  | L2 Weight | L1 Features | L1 Weight |\n",
      "+------+---------------+-----------+-------------+-----------+\n",
      "|  1   |    windows    |  -0.7514  |   windows   |  -2.6773  |\n",
      "|  2   |      file     |   -0.284  |    window   |  -0.7411  |\n",
      "|  3   |     window    |  -0.2658  |    files    |  -0.5714  |\n",
      "|  4   |      win3     |  -0.2561  |     file    |  -0.5552  |\n",
      "|  5   |      win      |  -0.2424  |    bj200    |  -0.5214  |\n",
      "|  6   |       nt      |  -0.2287  |     cica    |  -0.4608  |\n",
      "|  7   |   protected   |   0.2129  |    board    |   0.4477  |\n",
      "|  8   | manufacturers |   0.2066  |     bus     |   0.4415  |\n",
      "|  9   |     files     |  -0.2014  |     win3    |  -0.4331  |\n",
      "|  10  |    monitor    |   0.1958  |   rutgers   |  -0.4327  |\n",
      "|  11  |    download   |  -0.1956  |     000     |  -0.4305  |\n",
      "|  12  |    geometry   |   0.1855  |     ide     |   0.4301  |\n",
      "|  13  |    gateway    |   0.1787  |     ibm     |   0.4279  |\n",
      "|  14  |      bus      |   0.1772  |   haapanen  |  -0.4189  |\n",
      "|  15  |      ibm      |   0.1763  |    latest   |  -0.4184  |\n",
      "|  16  |      for      |  -0.1757  |    drive    |   0.4175  |\n",
      "|  17  |     driver    |  -0.1754  |    wayne    |   0.3985  |\n",
      "|  18  |   microsoft   |  -0.1753  |     win     |  -0.3984  |\n",
      "|  19  |  motherboard  |   0.1746  |   remains   |   0.3812  |\n",
      "|  20  |   controller  |   0.1734  |   mitchell  |  -0.3799  |\n",
      "+------+---------------+-----------+-------------+-----------+\n",
      " \n",
      "List of features in both L1 and L2 penalty :\n",
      "\t 1 windows\n",
      "\t 2 window\n",
      "\t 3 files\n",
      "\t 4 file\n",
      "\t 5 bus\n",
      "\t 6 win3\n",
      "\t 7 ibm\n",
      "\t 8 win\n"
     ]
    }
   ],
   "source": [
    "print_features(clf_l1, clf_l2, f_names, 'with Z-score scaling', iter_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 : 0.860050890585\n",
      "L1 : 0.81679389313\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l2.predict(X_test)\n",
    "y_l1 = clf_l1.predict(X_test)\n",
    "\n",
    "\n",
    "print('L2 :', accuracy_score(y_pred, y_test))\n",
    "print('L1 :', accuracy_score(y_l1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(786, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = clf_l2.predict_proba(X_test_scaled)\n",
    "y_pred_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(786, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_log_proba = clf_l2.predict_log_proba(X_test_scaled)\n",
    "y_pred_log_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_object_evidence(msg, obj_idx, X_test, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10):\n",
    "    print(msg)\n",
    "    print(' ')\n",
    "    print('Index of the object : ', obj_idx)\n",
    "    print(X_test[obj_idx, :])\n",
    "    print('Class : ', y_test[obj_idx])\n",
    "    print('Predict Class : ', y_pred[obj_idx])\n",
    "\n",
    "    if y_test[obj_idx] != y_pred[obj_idx]:\n",
    "        print('----------False Positive-------------')\n",
    "        print(' ')\n",
    "\n",
    "    print('a) Total  positive evidence : ', pos_ev[obj_idx])\n",
    "    print('b) Total negative evidence : ', neg_ev[obj_idx])\n",
    "    print('c) Probability distribution', y_pred_proba[obj_idx])\n",
    "\n",
    "    feature_pos = X_ev[obj_idx,:]\n",
    "    pos_list = np.argsort(feature_pos)[::-1]\n",
    "    feature_neg = X_ev[obj_idx,:]\n",
    "    neg_list = np.argsort(feature_neg)\n",
    "\n",
    "    print('d) Top', iter_range, 'features values that contribute most to the positive evidence')\n",
    "    for i in range(0,iter_range):\n",
    "    #    print('\\t',pos_list[i], '\\t Evidence Value : ', np.sort(feature_pos)[::-1][i])\n",
    "        print('\\t',f_names[pos_list[i]], '\\t Value: ', X_test[obj_idx, pos_list[i]], '\\t Evidence: ', np.sort(feature_pos)[::-1][i], '\\tCoef_: ', coef_[0, pos_list[i]])   \n",
    "\n",
    "    print('e) Top', iter_range, 'features values that contribute most to the negative evidence')\n",
    "    for j in range(0,iter_range):\n",
    "    #    print('\\t',neg_list[j], '\\t Evidence Value : ', np.sort(feature_neg)[j])\n",
    "        print('\\t',f_names[neg_list[j]], '\\t Value: ', X_test[uncertain_idx, neg_list[j]], '\\t Evidence: ', np.sort(feature_neg)[j], '\\tCoef_: ', coef_[0, pos_list[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Copy the data X and coefficient w_i'''\n",
    "X = np.copy(X_test_scaled)\n",
    "coef_ = np.copy(clf_l2.coef_)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "'''Initialize X_ev : x_i * w_i'''\n",
    "X_ev = np.zeros((n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Get evidence'''\n",
    "'''Calculate the w_ia_i'''\n",
    "for idx in range(n_samples):\n",
    "    X_ev[idx, :] = X[idx,:] * coef_\n",
    "\n",
    "'''Generate the sets of P and N'''\n",
    "X_pos_ev = X_ev * (X_ev > 0)\n",
    "X_neg_ev = X_ev * (X_ev < 0)\n",
    "\n",
    "'''Sum each the set P and N'''\n",
    "pos_ev = np.sum(X_pos_ev, axis=1)\n",
    "neg_ev = np.sum(X_neg_ev, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most positive object with respect to the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_pos_obj_idx = np.argmax(y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Object w.r.t Probabilities\n",
      " \n",
      "Index of the object :  537\n",
      "[-0.18479884 -0.09241055 -0.12440745 ..., -0.06520507 -0.06520507\n",
      " -0.06520507]\n",
      "Class :  1\n",
      "Predict Class :  1\n",
      "a) Total  positive evidence :  39.3744025942\n",
      "b) Total negative evidence :  -19.5166927197\n",
      "c) Probability distribution [  2.29971975e-09   9.99999998e-01]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "\t randy \t Value:  12.9504550389 \t Evidence:  1.02727499381 \tCoef_:  0.0793234670693\n",
      "\t toshiba \t Value:  9.47872110815 \t Evidence:  0.829930456686 \tCoef_:  0.0875572186602\n",
      "\t msc \t Value:  12.9504550389 \t Evidence:  0.809522461873 \tCoef_:  0.0625091905606\n",
      "\t range \t Value:  10.3132747643 \t Evidence:  0.74599367076 \tCoef_:  0.0723333458878\n",
      "\t austin \t Value:  5.9921824071 \t Evidence:  0.724198341315 \tCoef_:  0.120857192274\n",
      "\t comments \t Value:  5.41128924398 \t Evidence:  0.703637109718 \tCoef_:  0.130031324883\n",
      "\t 1542b \t Value:  12.9504550389 \t Evidence:  0.659264972351 \tCoef_:  0.0509067033067\n",
      "\t currently \t Value:  5.20759518579 \t Evidence:  0.650459227702 \tCoef_:  0.12490587392\n",
      "\t windows \t Value:  -0.850933364748 \t Evidence:  0.63939851851 \tCoef_:  -0.751408447475\n",
      "\t seek \t Value:  12.1088810383 \t Evidence:  0.637132697734 \tCoef_:  0.0526169755668\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "\t standards \t Value:  -0.0652050663697 \t Evidence:  -1.29537890871 \tCoef_:  0.0526169755668\n",
      "\t both \t Value:  -0.340964937856 \t Evidence:  -0.361774749093 \tCoef_:  0.0526169755668\n",
      "\t average \t Value:  -0.0876309356755 \t Evidence:  -0.354546876872 \tCoef_:  0.0526169755668\n",
      "\t 1993apr22 \t Value:  -0.0969624123138 \t Evidence:  -0.33250628278 \tCoef_:  0.0526169755668\n",
      "\t obviously \t Value:  -0.101317243932 \t Evidence:  -0.326382661769 \tCoef_:  0.0526169755668\n",
      "\t originator \t Value:  -0.169545405716 \t Evidence:  -0.324629461693 \tCoef_:  0.0526169755668\n",
      "\t group \t Value:  -0.294068939091 \t Evidence:  -0.316632527391 \tCoef_:  0.0526169755668\n",
      "\t name \t Value:  -0.237382868207 \t Evidence:  -0.257112767471 \tCoef_:  0.0526169755668\n",
      "\t along \t Value:  -0.158661939705 \t Evidence:  -0.256193439682 \tCoef_:  0.0526169755668\n",
      "\t benefit \t Value:  -0.101317243932 \t Evidence:  -0.25294529192 \tCoef_:  0.0526169755668\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Most Positive Object w.r.t Probabilities', \n",
    "                      most_pos_obj_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most negative object with respect to the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_neg_obj_idx = np.argmin(y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Negative Object w.r.t Probabilities\n",
      " \n",
      "Index of the object :  19\n",
      "[-0.18479884 -0.09241055 -0.12440745 ..., -0.06520507 -0.06520507\n",
      " -0.06520507]\n",
      "Class :  0\n",
      "Predict Class :  0\n",
      "a) Total  positive evidence :  66.1733954293\n",
      "b) Total negative evidence :  -85.3131487893\n",
      "c) Probability distribution [  9.99999995e-01   5.03433350e-09]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "\t protected \t Value:  9.86998817966 \t Evidence:  2.10130061853 \tCoef_:  0.212897987341\n",
      "\t interrupt \t Value:  8.53302408294 \t Evidence:  1.40005785697 \tCoef_:  0.164075226246\n",
      "\t instructions \t Value:  9.47872110815 \t Evidence:  1.2207758356 \tCoef_:  0.128791196794\n",
      "\t provided \t Value:  11.4114951791 \t Evidence:  1.15212231226 \tCoef_:  0.100961556236\n",
      "\t ethernet \t Value:  9.86998817966 \t Evidence:  1.10164379596 \tCoef_:  0.1116155132\n",
      "\t wish \t Value:  8.81665091366 \t Evidence:  0.942156430895 \tCoef_:  0.106861033756\n",
      "\t utexas \t Value:  7.82035131799 \t Evidence:  0.922859760274 \tCoef_:  0.118007455516\n",
      "\t relevant \t Value:  12.1088810383 \t Evidence:  0.821976770283 \tCoef_:  0.0678821410238\n",
      "\t burn \t Value:  10.3132747643 \t Evidence:  0.816932003306 \tCoef_:  0.0792116977369\n",
      "\t hit \t Value:  8.53302408294 \t Evidence:  0.798667205009 \tCoef_:  0.093597205076\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "\t winqvt \t Value:  -0.0924105494512 \t Evidence:  -1.55136437325 \tCoef_:  0.093597205076\n",
      "\t 255 \t Value:  -0.0714589601011 \t Evidence:  -1.52437945185 \tCoef_:  0.093597205076\n",
      "\t kermit \t Value:  -0.0772173639457 \t Evidence:  -1.43124926674 \tCoef_:  0.093597205076\n",
      "\t document \t Value:  -0.0969624123138 \t Evidence:  -1.35649986716 \tCoef_:  0.093597205076\n",
      "\t pif \t Value:  -0.0924105494512 \t Evidence:  -1.16568032875 \tCoef_:  0.093597205076\n",
      "\t window \t Value:  -0.239366516986 \t Evidence:  -1.11040132051 \tCoef_:  0.093597205076\n",
      "\t terminal \t Value:  -0.0652050663697 \t Evidence:  -1.09628195405 \tCoef_:  0.093597205076\n",
      "\t script \t Value:  -0.0652050663697 \t Evidence:  -1.08631007699 \tCoef_:  0.093597205076\n",
      "\t characters \t Value:  -0.152960388388 \t Evidence:  -0.996114873899 \tCoef_:  0.093597205076\n",
      "\t trumpet \t Value:  -0.0825840139015 \t Evidence:  -0.973323859035 \tCoef_:  0.093597205076\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Most Negative Object w.r.t Probabilities', \n",
    "                      most_neg_obj_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The object that has the largest positive evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_pos_ev_idx = np.argmax(pos_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object with Largest Positive Evidence\n",
      " \n",
      "Index of the object :  19\n",
      "[-0.18479884 -0.09241055 -0.12440745 ..., -0.06520507 -0.06520507\n",
      " -0.06520507]\n",
      "Class :  0\n",
      "Predict Class :  0\n",
      "a) Total  positive evidence :  66.1733954293\n",
      "b) Total negative evidence :  -85.3131487893\n",
      "c) Probability distribution [  9.99999995e-01   5.03433350e-09]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "\t protected \t Value:  9.86998817966 \t Evidence:  2.10130061853 \tCoef_:  0.212897987341\n",
      "\t interrupt \t Value:  8.53302408294 \t Evidence:  1.40005785697 \tCoef_:  0.164075226246\n",
      "\t instructions \t Value:  9.47872110815 \t Evidence:  1.2207758356 \tCoef_:  0.128791196794\n",
      "\t provided \t Value:  11.4114951791 \t Evidence:  1.15212231226 \tCoef_:  0.100961556236\n",
      "\t ethernet \t Value:  9.86998817966 \t Evidence:  1.10164379596 \tCoef_:  0.1116155132\n",
      "\t wish \t Value:  8.81665091366 \t Evidence:  0.942156430895 \tCoef_:  0.106861033756\n",
      "\t utexas \t Value:  7.82035131799 \t Evidence:  0.922859760274 \tCoef_:  0.118007455516\n",
      "\t relevant \t Value:  12.1088810383 \t Evidence:  0.821976770283 \tCoef_:  0.0678821410238\n",
      "\t burn \t Value:  10.3132747643 \t Evidence:  0.816932003306 \tCoef_:  0.0792116977369\n",
      "\t hit \t Value:  8.53302408294 \t Evidence:  0.798667205009 \tCoef_:  0.093597205076\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "\t winqvt \t Value:  -0.0924105494512 \t Evidence:  -1.55136437325 \tCoef_:  0.093597205076\n",
      "\t 255 \t Value:  -0.0714589601011 \t Evidence:  -1.52437945185 \tCoef_:  0.093597205076\n",
      "\t kermit \t Value:  -0.0772173639457 \t Evidence:  -1.43124926674 \tCoef_:  0.093597205076\n",
      "\t document \t Value:  -0.0969624123138 \t Evidence:  -1.35649986716 \tCoef_:  0.093597205076\n",
      "\t pif \t Value:  -0.0924105494512 \t Evidence:  -1.16568032875 \tCoef_:  0.093597205076\n",
      "\t window \t Value:  -0.239366516986 \t Evidence:  -1.11040132051 \tCoef_:  0.093597205076\n",
      "\t terminal \t Value:  -0.0652050663697 \t Evidence:  -1.09628195405 \tCoef_:  0.093597205076\n",
      "\t script \t Value:  -0.0652050663697 \t Evidence:  -1.08631007699 \tCoef_:  0.093597205076\n",
      "\t characters \t Value:  -0.152960388388 \t Evidence:  -0.996114873899 \tCoef_:  0.093597205076\n",
      "\t trumpet \t Value:  -0.0825840139015 \t Evidence:  -0.973323859035 \tCoef_:  0.093597205076\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Object with Largest Positive Evidence', \n",
    "                      most_neg_obj_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The object that has the largest negative evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_neg_ev_idx = np.argmin(neg_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object with Largest Negative Evidence\n",
      " \n",
      "Index of the object :  353\n",
      "[  5.41128924  -0.09241055   8.0381037  ...,  15.33623161  15.33623161\n",
      "  15.33623161]\n",
      "Class :  0\n",
      "Predict Class :  0\n",
      "a) Total  positive evidence :  95.3842545282\n",
      "b) Total negative evidence :  -107.430355622\n",
      "c) Probability distribution [  9.99993937e-01   6.06280202e-06]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "\t ra \t Value:  11.4114951791 \t Evidence:  1.80035574664 \tCoef_:  0.157766858627\n",
      "\t el \t Value:  10.8212753407 \t Evidence:  1.14690051354 \tCoef_:  0.105985706622\n",
      "\t ts \t Value:  15.3362316101 \t Evidence:  1.06539737628 \tCoef_:  0.0694693066302\n",
      "\t ___ \t Value:  8.03810370119 \t Evidence:  1.04404602503 \tCoef_:  0.129887105695\n",
      "\t wang \t Value:  11.4114951791 \t Evidence:  0.922542533996 \tCoef_:  0.0808432654545\n",
      "\t amd \t Value:  10.8212753407 \t Evidence:  0.896234375247 \tCoef_:  0.0828215110536\n",
      "\t yo \t Value:  11.4114951791 \t Evidence:  0.895066654503 \tCoef_:  0.0784355284261\n",
      "\t sb \t Value:  8.03810370119 \t Evidence:  0.892214761524 \tCoef_:  0.110998165076\n",
      "\t tue \t Value:  10.8212753407 \t Evidence:  0.882917385771 \tCoef_:  0.0815908807391\n",
      "\t btw \t Value:  6.8 \t Evidence:  0.743056827408 \tCoef_:  0.109273062854\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "\t tn \t Value:  -0.0714589601011 \t Evidence:  -1.90074250314 \tCoef_:  0.109273062854\n",
      "\t ev \t Value:  -0.0652050663697 \t Evidence:  -1.46719205134 \tCoef_:  0.109273062854\n",
      "\t p7 \t Value:  -0.0825840139015 \t Evidence:  -1.35192233588 \tCoef_:  0.109273062854\n",
      "\t vb \t Value:  -0.0876309356755 \t Evidence:  -1.1140668499 \tCoef_:  0.109273062854\n",
      "\t nt \t Value:  -0.212444683066 \t Evidence:  -1.07654948734 \tCoef_:  0.109273062854\n",
      "\t 0c \t Value:  -0.0876309356755 \t Evidence:  -1.00166375661 \tCoef_:  0.109273062854\n",
      "\t kx \t Value:  -0.0825840139015 \t Evidence:  -0.922298600775 \tCoef_:  0.109273062854\n",
      "\t aw \t Value:  -0.0924105494512 \t Evidence:  -0.913618873364 \tCoef_:  0.109273062854\n",
      "\t cl \t Value:  -0.109528865638 \t Evidence:  -0.907284792035 \tCoef_:  0.109273062854\n",
      "\t hal \t Value:  -0.0876309356755 \t Evidence:  -0.905933720113 \tCoef_:  0.109273062854\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Object with Largest Negative Evidence', \n",
    "                      most_neg_ev_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most uncertain object with respect to the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uncertain_idx = np.argmin(np.square(y_pred_proba[:,1]-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most uncertain object\n",
      " \n",
      "Index of the object :  379\n",
      "[-0.18479884 -0.09241055 -0.12440745 ..., -0.06520507 -0.06520507\n",
      " -0.06520507]\n",
      "Class :  1\n",
      "Predict Class :  1\n",
      "a) Total  positive evidence :  19.3728532003\n",
      "b) Total negative evidence :  -19.4071614659\n",
      "c) Probability distribution [ 0.5003849  0.4996151]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "\t irqs \t Value:  12.9504550389 \t Evidence:  0.763230439448 \tCoef_:  0.0589346426173\n",
      "\t direction \t Value:  11.4114951791 \t Evidence:  0.755287249742 \tCoef_:  0.0661865283987\n",
      "\t manuals \t Value:  9.86998817966 \t Evidence:  0.699173966101 \tCoef_:  0.070838379274\n",
      "\t motherboard \t Value:  3.84013888638 \t Evidence:  0.670654543096 \tCoef_:  0.174643304042\n",
      "\t windows \t Value:  -0.850933364748 \t Evidence:  0.63939851851 \tCoef_:  -0.751408447475\n",
      "\t hook \t Value:  9.47872110815 \t Evidence:  0.597926043425 \tCoef_:  0.0630808773253\n",
      "\t ports \t Value:  5.80820920058 \t Evidence:  0.487780193577 \tCoef_:  0.0839811681591\n",
      "\t hello \t Value:  4.52467979781 \t Evidence:  0.482861216765 \tCoef_:  0.106717212785\n",
      "\t port \t Value:  3.84013888638 \t Evidence:  0.432489319258 \tCoef_:  0.112623353492\n",
      "\t various \t Value:  6.53764030371 \t Evidence:  0.332785329624 \tCoef_:  0.0509029732693\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "\t vnews \t Value:  10.3132747643 \t Evidence:  -0.823671686646 \tCoef_:  0.0509029732693\n",
      "\t norton \t Value:  5.34088007729 \t Evidence:  -0.718932294291 \tCoef_:  0.0509029732693\n",
      "\t basic \t Value:  8.03810370119 \t Evidence:  -0.629724993025 \tCoef_:  0.0509029732693\n",
      "\t diagnostics \t Value:  12.1088810383 \t Evidence:  -0.561519949995 \tCoef_:  0.0509029732693\n",
      "\t msd \t Value:  15.3362316101 \t Evidence:  -0.520949307819 \tCoef_:  0.0509029732693\n",
      "\t massachusetts \t Value:  9.47872110815 \t Evidence:  -0.470871762337 \tCoef_:  0.0509029732693\n",
      "\t 41 \t Value:  6.19408319824 \t Evidence:  -0.457454283516 \tCoef_:  0.0509029732693\n",
      "\t vms \t Value:  6.53764030371 \t Evidence:  -0.402178751813 \tCoef_:  0.0509029732693\n",
      "\t checked \t Value:  7.43223352957 \t Evidence:  -0.354811571747 \tCoef_:  0.0509029732693\n",
      "\t programs \t Value:  4.01446659333 \t Evidence:  -0.317746828543 \tCoef_:  0.0509029732693\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('The most uncertain object', \n",
    "                      uncertain_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
