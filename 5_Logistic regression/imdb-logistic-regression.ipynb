{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using IMDB Reviews (large) datasets\n",
    "\n",
    "The source of this dataset can be accessed through the following link: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "This dataset provides data for train and test. Each of dataset contains 25,000 instances.\n",
    "\n",
    "In this program, we are using the Count Vectorizer to count the text frequency given in the datasets. Thus, for the sake of memory limitation, we limited the maximum number of feature by 10,000. In this way, we could transform the matrix to dense for further scaling the data using the StandardScaler by sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import graphviz\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from prettytable import PrettyTable\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path):\n",
    "    \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb data\n",
      "Train Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus , y_train, X_test_corpus , y_test = load_imdb('./aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_features(clf_L1, clf_L2, f_names, msg, iter_range=10):\n",
    "    idx_L2 = np.argsort(np.absolute(clf_L2.coef_)[0,:])[::-1]\n",
    "    idx_L1 = np.argsort(np.absolute(clf_L1.coef_)[0,:])[::-1]\n",
    "    \n",
    "    ### Print on Pretty Table\n",
    "    table_features = PrettyTable(['Rank', 'L2 Features', 'L2 Weight', 'L1 Features', 'L1 Weight'])\n",
    "    f_list_1 = np.zeros(iter_range, dtype='int16')\n",
    "    f_list_2 = np.ones(iter_range, dtype='int16')\n",
    "    \n",
    "    for idx in range(0,iter_range):\n",
    "        table_features.add_row([idx+1, \n",
    "                                f_names[idx_L2[idx]], \n",
    "                                np.around(clf_L2.coef_[0,idx_L2[idx]], decimals=4), \n",
    "                                f_names[idx_L1[idx]], \n",
    "                                np.around(clf_L1.coef_[0,idx_L1[idx]], decimals=4)])\n",
    "        f_list_2[idx] = idx_L2[idx]\n",
    "        f_list_1[idx] = idx_L1[idx]\n",
    "        \n",
    "    print('L2 and L1-regularized Logistic Regression Classifier', msg)\n",
    "    print('Top 10 features and weights (with absolute value)')\n",
    "    print()\n",
    "    print(table_features)\n",
    "    print(' ')\n",
    "    print('List of features in both L1 and L2 penalty :')\n",
    "    num=1\n",
    "    for i in range(0,iter_range):\n",
    "        for j in range(0,iter_range):\n",
    "            if f_list_1[i] == f_list_2[j]:\n",
    "                print('\\t', num, f_names[f_list_1[i]])\n",
    "                num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Vectorizer Transform start\n",
      "\n",
      "Train Data Transformed\n",
      "Train Data size  (25000, 10000)\n",
      "\n",
      "Test Data Transformed\n",
      "Test Data size  (25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=1.0, min_df=2, max_features = 10000, binary=True)\n",
    "\n",
    "print('Data Vectorizer Transform start')\n",
    "print()\n",
    "X_train = tf_vectorizer.fit_transform(X_train_corpus)\n",
    "\n",
    "print('Train Data Transformed')\n",
    "print('Train Data size ', X_train.shape)\n",
    "print()\n",
    "X_test = tf_vectorizer.transform(X_test_corpus)\n",
    "print('Test Data Transformed')\n",
    "print('Test Data size ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model to the Data without Z-score scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "clf_noz_l2 = LogisticRegression()\n",
    "clf_noz_l2.fit(X_train, y_train)\n",
    "print(clf_noz_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_noz_l2 = np.argsort(np.absolute(clf_noz_l2.coef_)[0,:])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_noz_l1 = LogisticRegression(penalty='l1')\n",
    "clf_noz_l1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_noz_l1 = np.argsort(np.absolute(clf_noz_l1.coef_)[0,:])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 and L1-regularized Logistic Regression Classifier without Z-score scaling\n",
      "Top 10 features and weights (with absolute value)\n",
      "\n",
      "+------+----------------+-----------+--------------+-----------+\n",
      "| Rank |  L2 Features   | L2 Weight | L1 Features  | L1 Weight |\n",
      "+------+----------------+-----------+--------------+-----------+\n",
      "|  1   |     waste      |   -2.376  |    wayans    |  -4.0908  |\n",
      "|  2   |     worst      |  -2.3141  |  vengeance   |   3.245   |\n",
      "|  3   | disappointment |  -2.2132  |    greene    |   3.0827  |\n",
      "|  4   |     poorly     |  -2.0917  |   cerebral   |   3.0473  |\n",
      "|  5   |    unfunny     |  -1.9441  | abomination  |  -2.9896  |\n",
      "|  6   |     refer      |  -1.8773  |    finely    |   2.8893  |\n",
      "|  7   |   vengeance    |   1.8422  |   lifeless   |  -2.8084  |\n",
      "|  8   |  appreciated   |   1.8079  |     café     |   2.7899  |\n",
      "|  9   |    cerebral    |   1.7976  | unremarkable |  -2.7801  |\n",
      "|  10  |   laughable    |  -1.7907  |    refer     |   -2.712  |\n",
      "|  11  |   refreshing   |   1.782   |  unlikeable  |  -2.6512  |\n",
      "|  12  |     hooked     |   1.7494  |  uneducated  |  -2.6096  |\n",
      "|  13  |     awful      |  -1.7432  |    kitty     |   2.5957  |\n",
      "|  14  |    baldwin     |  -1.7183  |   sources    |   -2.548  |\n",
      "|  15  |     greene     |   1.7028  |   believer   |  -2.5428  |\n",
      "|  16  |  forgettable   |  -1.6871  |    capote    |   2.4875  |\n",
      "|  17  |     lacks      |  -1.6539  |   baldwin    |   -2.483  |\n",
      "|  18  |    sources     |  -1.6319  |     yawn     |  -2.4639  |\n",
      "|  19  |     wayans     |  -1.6149  | outrageously |   2.4285  |\n",
      "|  20  |     kitty      |   1.5979  |    pistol    |  -2.4215  |\n",
      "+------+----------------+-----------+--------------+-----------+\n",
      " \n",
      "List of features in both L1 and L2 penalty :\n",
      "\t 1 wayans\n",
      "\t 2 vengeance\n",
      "\t 3 greene\n",
      "\t 4 cerebral\n",
      "\t 5 refer\n",
      "\t 6 kitty\n",
      "\t 7 sources\n",
      "\t 8 baldwin\n"
     ]
    }
   ],
   "source": [
    "print_features(clf_noz_l1, clf_noz_l2, f_names, 'without Z-score scaling', iter_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"719pt\" height=\"685pt\"\r\n",
       " viewBox=\"0.00 0.00 719.00 685.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 681)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-681 715,-681 715,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M622.5,-677C622.5,-677 483.5,-677 483.5,-677 477.5,-677 471.5,-671 471.5,-665 471.5,-665 471.5,-621 471.5,-621 471.5,-615 477.5,-609 483.5,-609 483.5,-609 622.5,-609 622.5,-609 628.5,-609 634.5,-615 634.5,-621 634.5,-621 634.5,-665 634.5,-665 634.5,-671 628.5,-677 622.5,-677\"/>\r\n",
       "<text text-anchor=\"start\" x=\"523\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">bad ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"524\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"497\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25000</text>\r\n",
       "<text text-anchor=\"start\" x=\"479.5\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [12500, 12500]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.262745\" stroke=\"black\" d=\"M534.5,-573C534.5,-573 403.5,-573 403.5,-573 397.5,-573 391.5,-567 391.5,-561 391.5,-561 391.5,-517 391.5,-517 391.5,-511 397.5,-505 403.5,-505 403.5,-505 534.5,-505 534.5,-505 540.5,-505 546.5,-511 546.5,-517 546.5,-517 546.5,-561 546.5,-561 546.5,-567 540.5,-573 534.5,-573\"/>\r\n",
       "<text text-anchor=\"start\" x=\"434\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">worst ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"431.5\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.488</text>\r\n",
       "<text text-anchor=\"start\" x=\"413\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 19096</text>\r\n",
       "<text text-anchor=\"start\" x=\"399.5\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [8094, 11002]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M525.728,-608.884C518.465,-600.065 510.531,-590.43 502.958,-581.235\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"505.482,-578.794 496.423,-573.299 500.078,-583.244 505.482,-578.794\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"494.015\" y=\"-594.483\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.658824\" stroke=\"black\" d=\"M699,-565.5C699,-565.5 577,-565.5 577,-565.5 571,-565.5 565,-559.5 565,-553.5 565,-553.5 565,-524.5 565,-524.5 565,-518.5 571,-512.5 577,-512.5 577,-512.5 699,-512.5 699,-512.5 705,-512.5 711,-518.5 711,-524.5 711,-524.5 711,-553.5 711,-553.5 711,-559.5 705,-565.5 699,-565.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"600.5\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.379</text>\r\n",
       "<text text-anchor=\"start\" x=\"586.5\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5904</text>\r\n",
       "<text text-anchor=\"start\" x=\"573\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4406, 1498]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>0&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M580.597,-608.884C590.037,-597.556 600.6,-584.88 610.066,-573.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"613.02,-575.443 616.733,-565.52 607.643,-570.962 613.02,-575.443\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"618.997\" y=\"-586.717\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.349020\" stroke=\"black\" d=\"M452.5,-469C452.5,-469 321.5,-469 321.5,-469 315.5,-469 309.5,-463 309.5,-457 309.5,-457 309.5,-413 309.5,-413 309.5,-407 315.5,-401 321.5,-401 321.5,-401 452.5,-401 452.5,-401 458.5,-401 464.5,-407 464.5,-413 464.5,-413 464.5,-457 464.5,-457 464.5,-463 458.5,-469 452.5,-469\"/>\r\n",
       "<text text-anchor=\"start\" x=\"350.5\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">waste ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"349.5\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.477</text>\r\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 17866</text>\r\n",
       "<text text-anchor=\"start\" x=\"317.5\" y=\"-408.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [7032, 10834]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M442.377,-504.884C435.288,-496.065 427.542,-486.43 420.15,-477.235\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"422.764,-474.9 413.77,-469.299 417.308,-479.286 422.764,-474.9\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.843137\" stroke=\"black\" d=\"M609,-461.5C609,-461.5 495,-461.5 495,-461.5 489,-461.5 483,-455.5 483,-449.5 483,-449.5 483,-420.5 483,-420.5 483,-414.5 489,-408.5 495,-408.5 495,-408.5 609,-408.5 609,-408.5 615,-408.5 621,-414.5 621,-420.5 621,-420.5 621,-449.5 621,-449.5 621,-455.5 615,-461.5 609,-461.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"514.5\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.236</text>\r\n",
       "<text text-anchor=\"start\" x=\"500.5\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1230</text>\r\n",
       "<text text-anchor=\"start\" x=\"491\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1062, 168]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>1&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M495.948,-504.884C505.165,-493.556 515.48,-480.88 524.723,-469.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"527.637,-471.486 531.234,-461.52 522.207,-467.067 527.637,-471.486\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.400000\" stroke=\"black\" d=\"M374.5,-365C374.5,-365 243.5,-365 243.5,-365 237.5,-365 231.5,-359 231.5,-353 231.5,-353 231.5,-309 231.5,-309 231.5,-303 237.5,-297 243.5,-297 243.5,-297 374.5,-297 374.5,-297 380.5,-297 386.5,-303 386.5,-309 386.5,-309 386.5,-353 386.5,-353 386.5,-359 380.5,-365 374.5,-365\"/>\r\n",
       "<text text-anchor=\"start\" x=\"275\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">great ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"271.5\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.468</text>\r\n",
       "<text text-anchor=\"start\" x=\"253\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 17203</text>\r\n",
       "<text text-anchor=\"start\" x=\"239.5\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6438, 10765]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M361.676,-400.884C355,-392.154 347.713,-382.625 340.746,-373.514\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"343.319,-371.117 334.464,-365.299 337.758,-375.369 343.319,-371.117\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.882353\" stroke=\"black\" d=\"M515,-357.5C515,-357.5 417,-357.5 417,-357.5 411,-357.5 405,-351.5 405,-345.5 405,-345.5 405,-316.5 405,-316.5 405,-310.5 411,-304.5 417,-304.5 417,-304.5 515,-304.5 515,-304.5 521,-304.5 527,-310.5 527,-316.5 527,-316.5 527,-345.5 527,-345.5 527,-351.5 521,-357.5 515,-357.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"428.5\" y=\"-342.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.186</text>\r\n",
       "<text text-anchor=\"start\" x=\"418.5\" y=\"-327.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 663</text>\r\n",
       "<text text-anchor=\"start\" x=\"413\" y=\"-312.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [594, 69]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>2&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M412.649,-400.884C421.423,-389.556 431.24,-376.88 440.038,-365.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"442.878,-367.569 446.235,-357.52 437.344,-363.283 442.878,-367.569\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.254902\" stroke=\"black\" d=\"M288,-261C288,-261 166,-261 166,-261 160,-261 154,-255 154,-249 154,-249 154,-205 154,-205 154,-199 160,-193 166,-193 166,-193 288,-193 288,-193 294,-193 300,-199 300,-205 300,-205 300,-249 300,-249 300,-255 294,-261 288,-261\"/>\r\n",
       "<text text-anchor=\"start\" x=\"193\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">awful ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"189.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.489</text>\r\n",
       "<text text-anchor=\"start\" x=\"171\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 12591</text>\r\n",
       "<text text-anchor=\"start\" x=\"162\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5372, 7219]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.377,-296.884C275.288,-288.065 267.542,-278.43 260.15,-269.235\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"262.764,-266.9 253.77,-261.299 257.308,-271.286 262.764,-266.9\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.698039\" stroke=\"black\" d=\"M452,-253.5C452,-253.5 330,-253.5 330,-253.5 324,-253.5 318,-247.5 318,-241.5 318,-241.5 318,-212.5 318,-212.5 318,-206.5 324,-200.5 330,-200.5 330,-200.5 452,-200.5 452,-200.5 458,-200.5 464,-206.5 464,-212.5 464,-212.5 464,-241.5 464,-241.5 464,-247.5 458,-253.5 452,-253.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"353.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.355</text>\r\n",
       "<text text-anchor=\"start\" x=\"339.5\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4612</text>\r\n",
       "<text text-anchor=\"start\" x=\"326\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1066, 3546]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>3&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M335.623,-296.884C344.73,-285.556 354.92,-272.88 364.052,-261.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"366.946,-263.507 370.484,-253.52 361.491,-259.121 366.946,-263.507\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.305882\" stroke=\"black\" d=\"M212,-157C212,-157 90,-157 90,-157 84,-157 78,-151 78,-145 78,-145 78,-101 78,-101 78,-95 84,-89 90,-89 90,-89 212,-89 212,-89 218,-89 224,-95 224,-101 224,-101 224,-145 224,-145 224,-151 218,-157 212,-157\"/>\r\n",
       "<text text-anchor=\"start\" x=\"113\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">boring ≤ 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.484</text>\r\n",
       "<text text-anchor=\"start\" x=\"95\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 12126</text>\r\n",
       "<text text-anchor=\"start\" x=\"86\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4972, 7154]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.325,-192.884C195.821,-184.154 188.72,-174.625 181.932,-165.514\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.593,-163.227 175.811,-157.299 178.98,-167.409 184.593,-163.227\"/>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.839216\" stroke=\"black\" d=\"M352,-149.5C352,-149.5 254,-149.5 254,-149.5 248,-149.5 242,-143.5 242,-137.5 242,-137.5 242,-108.5 242,-108.5 242,-102.5 248,-96.5 254,-96.5 254,-96.5 352,-96.5 352,-96.5 358,-96.5 364,-102.5 364,-108.5 364,-108.5 364,-137.5 364,-137.5 364,-143.5 358,-149.5 352,-149.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"269.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.24</text>\r\n",
       "<text text-anchor=\"start\" x=\"255.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 465</text>\r\n",
       "<text text-anchor=\"start\" x=\"250\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [400, 65]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>4&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.675,-192.884C260.033,-181.666 269.377,-169.126 277.777,-157.852\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.817,-159.63 283.985,-149.52 275.204,-155.448 280.817,-159.63\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.352941\" stroke=\"black\" d=\"M134,-53C134,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 134,-0 134,-0 140,-0 146,-6 146,-12 146,-12 146,-41 146,-41 146,-47 140,-53 134,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"35.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.477</text>\r\n",
       "<text text-anchor=\"start\" x=\"17\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 11549</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4544, 7005]</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.719,-88.9485C116.244,-79.892 108.136,-70.0682 100.64,-60.9875\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.308,-58.721 94.243,-53.2367 97.9094,-63.1769 103.308,-58.721\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.650980\" stroke=\"black\" d=\"M282,-53C282,-53 176,-53 176,-53 170,-53 164,-47 164,-41 164,-41 164,-12 164,-12 164,-6 170,-0 176,-0 176,-0 282,-0 282,-0 288,-0 294,-6 294,-12 294,-12 294,-41 294,-41 294,-47 288,-53 282,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"191.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.383</text>\r\n",
       "<text text-anchor=\"start\" x=\"181.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 577</text>\r\n",
       "<text text-anchor=\"start\" x=\"172\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [428, 149]</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.281,-88.9485C185.756,-79.892 193.864,-70.0682 201.36,-60.9875\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.091,-63.1769 207.757,-53.2367 198.692,-58.721 204.091,-63.1769\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x2ba5bb8f710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tree = DecisionTreeClassifier(max_depth=6, min_impurity_decrease=0.005)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "tree_plot = tree.export_graphviz(clf_tree, out_file=None, \n",
    "                                      feature_names=f_names, \n",
    "                                      filled=True, rounded=True, \n",
    "                                      special_characters=True) \n",
    "tree_graph_noz = graphviz.Source(tree_plot)\n",
    "tree_graph_noz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85908\n",
      "0.86388\n",
      "0.70412\n"
     ]
    }
   ],
   "source": [
    "y_pred_1 = clf_noz_l2.predict(X_test)\n",
    "print(accuracy_score(y_pred_1, y_test))\n",
    "y_pred_2 = clf_noz_l1.predict(X_test)\n",
    "print(accuracy_score(y_pred_2, y_test))\n",
    "y_pred_3 = clf_tree.predict(X_test)\n",
    "print(accuracy_score(y_pred_3, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Discussion\n",
    "## Original Data\n",
    "\n",
    "1. The accuracy score for Logistic regression with L2 penalty : 0.85908\n",
    "2. The accuracy score for Logistic regression with L1 penalty : 0.86392\n",
    "3. The accuracy score for Decision tree : 0.70412\n",
    "4. Regarding the top 10 words/features w.r.t weights/coefficient <br>\n",
    "The L1 and L2 penalty gives a different result on their weight. By looking on the table itself, only 3 features which intersect between L1 and L2. However, eventhough those features are intersected, it is hard to say that it indicates the similarity between both penalty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model to the Data with Z-score scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transform the X_train to dense\n",
    "X_train_dense = X_train.todense()\n",
    "X_test_dense = X_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_dense)\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test_dense)\n",
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l2 = LogisticRegression()\n",
    "clf_l2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#idx_l2 = np.argsort(np.absolute(clf_l2.coef_)[0,:])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l1 = LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#idx_l1 = np.argsort(np.absolute(clf_l1.coef_)[0,:])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 and L1-regularized Logistic Regression Classifier with Z-score scaling\n",
      "Top 10 features and weights (with absolute value)\n",
      "\n",
      "+------+----------------+-----------+----------------+-----------+\n",
      "| Rank |  L2 Features   | L2 Weight |  L1 Features   | L1 Weight |\n",
      "+------+----------------+-----------+----------------+-----------+\n",
      "|  1   |     worst      |  -1.3315  |     worst      |  -1.3357  |\n",
      "|  2   |      bad       |   -1.155  |     waste      |  -1.1687  |\n",
      "|  3   |     waste      |   -1.065  |      bad       |  -1.0952  |\n",
      "|  4   |     great      |   0.9473  |   excellent    |   0.8932  |\n",
      "|  5   |   excellent    |   0.9285  |     awful      |  -0.8444  |\n",
      "|  6   |     awful      |   -0.853  |     great      |   0.816   |\n",
      "|  7   |     boring     |  -0.8322  |    perfect     |   0.7098  |\n",
      "|  8   |    terrible    |  -0.7751  |     boring     |  -0.6994  |\n",
      "|  9   |      best      |   0.7082  |      best      |   0.6748  |\n",
      "|  10  |     worse      |  -0.7064  |     poorly     |  -0.6733  |\n",
      "|  11  |    perfect     |   0.7041  |    terrible    |  -0.6619  |\n",
      "|  12  |     poorly     |  -0.6736  |     worse      |  -0.6604  |\n",
      "|  13  |    nothing     |  -0.6597  | disappointment |  -0.6548  |\n",
      "|  14  |   wonderful    |   0.6583  |    nothing     |  -0.6511  |\n",
      "|  15  | disappointment |  -0.6451  |      mess      |  -0.6354  |\n",
      "|  16  |      poor      |  -0.6273  |   laughable    |  -0.5896  |\n",
      "|  17  |   enjoyable    |   0.6151  |     avoid      |  -0.5793  |\n",
      "|  18  |     avoid      |  -0.5745  |   wonderful    |   0.5779  |\n",
      "|  19  |     superb     |   0.5678  |    unfunny     |  -0.5764  |\n",
      "|  20  |   laughable    |  -0.5677  |     fails      |  -0.5721  |\n",
      "+------+----------------+-----------+----------------+-----------+\n",
      " \n",
      "List of features in both L1 and L2 penalty :\n",
      "\t 1 worst\n",
      "\t 2 waste\n",
      "\t 3 bad\n",
      "\t 4 excellent\n",
      "\t 5 awful\n",
      "\t 6 great\n",
      "\t 7 perfect\n",
      "\t 8 boring\n",
      "\t 9 best\n",
      "\t 10 poorly\n",
      "\t 11 terrible\n",
      "\t 12 worse\n",
      "\t 13 disappointment\n",
      "\t 14 nothing\n",
      "\t 15 laughable\n",
      "\t 16 avoid\n",
      "\t 17 wonderful\n"
     ]
    }
   ],
   "source": [
    "print_features(clf_l1, clf_l2, f_names, 'with Z-score scaling', iter_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"719pt\" height=\"685pt\"\r\n",
       " viewBox=\"0.00 0.00 719.00 685.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 681)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-681 715,-681 715,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M622.5,-677C622.5,-677 483.5,-677 483.5,-677 477.5,-677 471.5,-671 471.5,-665 471.5,-665 471.5,-621 471.5,-621 471.5,-615 477.5,-609 483.5,-609 483.5,-609 622.5,-609 622.5,-609 628.5,-609 634.5,-615 634.5,-621 634.5,-621 634.5,-665 634.5,-665 634.5,-671 628.5,-677 622.5,-677\"/>\r\n",
       "<text text-anchor=\"start\" x=\"515\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">bad ≤ 0.621</text>\r\n",
       "<text text-anchor=\"start\" x=\"524\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"497\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 25000</text>\r\n",
       "<text text-anchor=\"start\" x=\"479.5\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [12500, 12500]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.262745\" stroke=\"black\" d=\"M534.5,-573C534.5,-573 403.5,-573 403.5,-573 397.5,-573 391.5,-567 391.5,-561 391.5,-561 391.5,-517 391.5,-517 391.5,-511 397.5,-505 403.5,-505 403.5,-505 534.5,-505 534.5,-505 540.5,-505 546.5,-511 546.5,-517 546.5,-517 546.5,-561 546.5,-561 546.5,-567 540.5,-573 534.5,-573\"/>\r\n",
       "<text text-anchor=\"start\" x=\"426\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">worst ≤ 1.425</text>\r\n",
       "<text text-anchor=\"start\" x=\"431.5\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.488</text>\r\n",
       "<text text-anchor=\"start\" x=\"413\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 19096</text>\r\n",
       "<text text-anchor=\"start\" x=\"399.5\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [8094, 11002]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M525.728,-608.884C518.465,-600.065 510.531,-590.43 502.958,-581.235\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"505.482,-578.794 496.423,-573.299 500.078,-583.244 505.482,-578.794\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"494.015\" y=\"-594.483\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.658824\" stroke=\"black\" d=\"M699,-565.5C699,-565.5 577,-565.5 577,-565.5 571,-565.5 565,-559.5 565,-553.5 565,-553.5 565,-524.5 565,-524.5 565,-518.5 571,-512.5 577,-512.5 577,-512.5 699,-512.5 699,-512.5 705,-512.5 711,-518.5 711,-524.5 711,-524.5 711,-553.5 711,-553.5 711,-559.5 705,-565.5 699,-565.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"600.5\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.379</text>\r\n",
       "<text text-anchor=\"start\" x=\"586.5\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 5904</text>\r\n",
       "<text text-anchor=\"start\" x=\"573\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4406, 1498]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>0&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M580.597,-608.884C590.037,-597.556 600.6,-584.88 610.066,-573.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"613.02,-575.443 616.733,-565.52 607.643,-570.962 613.02,-575.443\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"618.997\" y=\"-586.717\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.349020\" stroke=\"black\" d=\"M452.5,-469C452.5,-469 321.5,-469 321.5,-469 315.5,-469 309.5,-463 309.5,-457 309.5,-457 309.5,-413 309.5,-413 309.5,-407 315.5,-401 321.5,-401 321.5,-401 452.5,-401 452.5,-401 458.5,-401 464.5,-407 464.5,-413 464.5,-413 464.5,-457 464.5,-457 464.5,-463 458.5,-469 452.5,-469\"/>\r\n",
       "<text text-anchor=\"start\" x=\"342.5\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">waste ≤ 2.014</text>\r\n",
       "<text text-anchor=\"start\" x=\"349.5\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.477</text>\r\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 17866</text>\r\n",
       "<text text-anchor=\"start\" x=\"317.5\" y=\"-408.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [7032, 10834]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M442.377,-504.884C435.288,-496.065 427.542,-486.43 420.15,-477.235\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"422.764,-474.9 413.77,-469.299 417.308,-479.286 422.764,-474.9\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.843137\" stroke=\"black\" d=\"M609,-461.5C609,-461.5 495,-461.5 495,-461.5 489,-461.5 483,-455.5 483,-449.5 483,-449.5 483,-420.5 483,-420.5 483,-414.5 489,-408.5 495,-408.5 495,-408.5 609,-408.5 609,-408.5 615,-408.5 621,-414.5 621,-420.5 621,-420.5 621,-449.5 621,-449.5 621,-455.5 615,-461.5 609,-461.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"514.5\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.236</text>\r\n",
       "<text text-anchor=\"start\" x=\"500.5\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1230</text>\r\n",
       "<text text-anchor=\"start\" x=\"491\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1062, 168]</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>1&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M495.948,-504.884C505.165,-493.556 515.48,-480.88 524.723,-469.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"527.637,-471.486 531.234,-461.52 522.207,-467.067 527.637,-471.486\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.400000\" stroke=\"black\" d=\"M374.5,-365C374.5,-365 243.5,-365 243.5,-365 237.5,-365 231.5,-359 231.5,-353 231.5,-353 231.5,-309 231.5,-309 231.5,-303 237.5,-297 243.5,-297 243.5,-297 374.5,-297 374.5,-297 380.5,-297 386.5,-303 386.5,-309 386.5,-309 386.5,-353 386.5,-353 386.5,-359 380.5,-365 374.5,-365\"/>\r\n",
       "<text text-anchor=\"start\" x=\"271\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">great ≤ 0.57</text>\r\n",
       "<text text-anchor=\"start\" x=\"271.5\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.468</text>\r\n",
       "<text text-anchor=\"start\" x=\"253\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 17203</text>\r\n",
       "<text text-anchor=\"start\" x=\"239.5\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6438, 10765]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M361.676,-400.884C355,-392.154 347.713,-382.625 340.746,-373.514\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"343.319,-371.117 334.464,-365.299 337.758,-375.369 343.319,-371.117\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.882353\" stroke=\"black\" d=\"M515,-357.5C515,-357.5 417,-357.5 417,-357.5 411,-357.5 405,-351.5 405,-345.5 405,-345.5 405,-316.5 405,-316.5 405,-310.5 411,-304.5 417,-304.5 417,-304.5 515,-304.5 515,-304.5 521,-304.5 527,-310.5 527,-316.5 527,-316.5 527,-345.5 527,-345.5 527,-351.5 521,-357.5 515,-357.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"428.5\" y=\"-342.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.186</text>\r\n",
       "<text text-anchor=\"start\" x=\"418.5\" y=\"-327.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 663</text>\r\n",
       "<text text-anchor=\"start\" x=\"413\" y=\"-312.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [594, 69]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>2&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M412.649,-400.884C421.423,-389.556 431.24,-376.88 440.038,-365.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"442.878,-367.569 446.235,-357.52 437.344,-363.283 442.878,-367.569\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.254902\" stroke=\"black\" d=\"M288,-261C288,-261 166,-261 166,-261 160,-261 154,-255 154,-249 154,-249 154,-205 154,-205 154,-199 160,-193 166,-193 166,-193 288,-193 288,-193 294,-193 300,-199 300,-205 300,-205 300,-249 300,-249 300,-255 294,-261 288,-261\"/>\r\n",
       "<text text-anchor=\"start\" x=\"184.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">awful ≤ 1.895</text>\r\n",
       "<text text-anchor=\"start\" x=\"189.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.489</text>\r\n",
       "<text text-anchor=\"start\" x=\"171\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 12591</text>\r\n",
       "<text text-anchor=\"start\" x=\"162\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [5372, 7219]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.377,-296.884C275.288,-288.065 267.542,-278.43 260.15,-269.235\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"262.764,-266.9 253.77,-261.299 257.308,-271.286 262.764,-266.9\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.698039\" stroke=\"black\" d=\"M452,-253.5C452,-253.5 330,-253.5 330,-253.5 324,-253.5 318,-247.5 318,-241.5 318,-241.5 318,-212.5 318,-212.5 318,-206.5 324,-200.5 330,-200.5 330,-200.5 452,-200.5 452,-200.5 458,-200.5 464,-206.5 464,-212.5 464,-212.5 464,-241.5 464,-241.5 464,-247.5 458,-253.5 452,-253.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"353.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.355</text>\r\n",
       "<text text-anchor=\"start\" x=\"339.5\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4612</text>\r\n",
       "<text text-anchor=\"start\" x=\"326\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1066, 3546]</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>3&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M335.623,-296.884C344.73,-285.556 354.92,-272.88 364.052,-261.521\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"366.946,-263.507 370.484,-253.52 361.491,-259.121 366.946,-263.507\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.305882\" stroke=\"black\" d=\"M212,-157C212,-157 90,-157 90,-157 84,-157 78,-151 78,-145 78,-145 78,-101 78,-101 78,-95 84,-89 90,-89 90,-89 212,-89 212,-89 218,-89 224,-95 224,-101 224,-101 224,-145 224,-145 224,-151 218,-157 212,-157\"/>\r\n",
       "<text text-anchor=\"start\" x=\"105\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">boring ≤ 1.846</text>\r\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.484</text>\r\n",
       "<text text-anchor=\"start\" x=\"95\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 12126</text>\r\n",
       "<text text-anchor=\"start\" x=\"86\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4972, 7154]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.325,-192.884C195.821,-184.154 188.72,-174.625 181.932,-165.514\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.593,-163.227 175.811,-157.299 178.98,-167.409 184.593,-163.227\"/>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.839216\" stroke=\"black\" d=\"M352,-149.5C352,-149.5 254,-149.5 254,-149.5 248,-149.5 242,-143.5 242,-137.5 242,-137.5 242,-108.5 242,-108.5 242,-102.5 248,-96.5 254,-96.5 254,-96.5 352,-96.5 352,-96.5 358,-96.5 364,-102.5 364,-108.5 364,-108.5 364,-137.5 364,-137.5 364,-143.5 358,-149.5 352,-149.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"269.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.24</text>\r\n",
       "<text text-anchor=\"start\" x=\"255.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 465</text>\r\n",
       "<text text-anchor=\"start\" x=\"250\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [400, 65]</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>4&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.675,-192.884C260.033,-181.666 269.377,-169.126 277.777,-157.852\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.817,-159.63 283.985,-149.52 275.204,-155.448 280.817,-159.63\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#399de5\" fill-opacity=\"0.352941\" stroke=\"black\" d=\"M134,-53C134,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 134,-0 134,-0 140,-0 146,-6 146,-12 146,-12 146,-41 146,-41 146,-47 140,-53 134,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"35.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.477</text>\r\n",
       "<text text-anchor=\"start\" x=\"17\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 11549</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4544, 7005]</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.719,-88.9485C116.244,-79.892 108.136,-70.0682 100.64,-60.9875\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.308,-58.721 94.243,-53.2367 97.9094,-63.1769 103.308,-58.721\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.650980\" stroke=\"black\" d=\"M282,-53C282,-53 176,-53 176,-53 170,-53 164,-47 164,-41 164,-41 164,-12 164,-12 164,-6 170,-0 176,-0 176,-0 282,-0 282,-0 288,-0 294,-6 294,-12 294,-12 294,-41 294,-41 294,-47 288,-53 282,-53\"/>\r\n",
       "<text text-anchor=\"start\" x=\"191.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.383</text>\r\n",
       "<text text-anchor=\"start\" x=\"181.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 577</text>\r\n",
       "<text text-anchor=\"start\" x=\"172\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [428, 149]</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.281,-88.9485C185.756,-79.892 193.864,-70.0682 201.36,-60.9875\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.091,-63.1769 207.757,-53.2367 198.692,-58.721 204.091,-63.1769\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x2ba57351048>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tree = DecisionTreeClassifier(max_depth=6, min_impurity_decrease=0.005)\n",
    "clf_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "tree_plot = tree.export_graphviz(clf_tree, out_file=None, \n",
    "                                      feature_names=f_names, \n",
    "                                      filled=True, rounded=True, \n",
    "                                      special_characters=True) \n",
    "tree_graph_noz = graphviz.Source(tree_plot)\n",
    "tree_graph_noz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8248\n",
      "0.83616\n",
      "0.70412\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l2.predict(X_test_scaled)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "y_pred_z = clf_l1.predict(X_test_scaled)\n",
    "print(accuracy_score(y_pred_z, y_test))\n",
    "y_pred_tree = clf_tree.predict(X_test_scaled)\n",
    "print(accuracy_score(y_pred_tree, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = clf_l2.predict_proba(X_test_scaled)\n",
    "y_pred_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1356: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.predict_proba(X))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_log_proba = clf_l2.predict_log_proba(X_test_scaled)\n",
    "y_pred_log_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l2.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.36362513])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Discussion\n",
    "## Z-score scaled data\n",
    "\n",
    "1. The accuracy score for Logistic regression with L2 penalty : 0.8248\n",
    "2. The accuracy score for Logistic regression with L1 penalty : 0.8362\n",
    "3. The accuracy score for Decision tree : 0.70412\n",
    "\n",
    "In the decision tree classifier, the tree graph shows the exactly same result according to the gini index. Here, it might because of the gini index does not change significantly when the data are scaled. Since it calculate the impurity of the item. \n",
    "<br>\n",
    "The number of features intersect between L1 and L2 penalty is higher compare to the result using the original data. Also the weight difference and the order are similar between those two. \n",
    "<br>\n",
    "Calculating by using scale data produce similar accuracy. However, the major drawbacks using the scaled data is the high consume in memory, since we need to treat the data in dense matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "### Calculating Evidence and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_object_evidence(msg, obj_idx, X_test, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10):\n",
    "    print(msg)\n",
    "    print(' ')\n",
    "    print('Index of the object : ', obj_idx)\n",
    "    print(X_test[obj_idx, :])\n",
    "    print('Class : ', y_test[obj_idx])\n",
    "    print('Predict Class : ', y_pred[obj_idx])\n",
    "\n",
    "    if y_test[obj_idx] != y_pred[obj_idx]:\n",
    "        print('----------False Positive-------------')\n",
    "        print(' ')\n",
    "\n",
    "    print('a) Total  positive evidence : ', pos_ev[obj_idx])\n",
    "    print('b) Total negative evidence : ', neg_ev[obj_idx])\n",
    "    print('c) Probability distribution', y_pred_proba[obj_idx])\n",
    "\n",
    "    feature_pos = X_ev[obj_idx,:]\n",
    "    pos_list = np.argsort(feature_pos)[::-1]\n",
    "    feature_neg = X_ev[obj_idx,:]\n",
    "    neg_list = np.argsort(feature_neg)\n",
    "\n",
    "    print('d) Top', iter_range, 'features values that contribute most to the positive evidence')\n",
    "    for i in range(0,iter_range):\n",
    "    #    print('\\t',pos_list[i], '\\t Evidence Value : ', np.sort(feature_pos)[::-1][i])\n",
    "        print(' ',i+1, f_names[pos_list[i]], '\\t Value: ', X_test[obj_idx, pos_list[i]], '\\t Evidence: ', np.sort(feature_pos)[::-1][i], '\\tCoef_: ', coef_[0, pos_list[i]])   \n",
    "\n",
    "    print('e) Top', iter_range, 'features values that contribute most to the negative evidence')\n",
    "    for j in range(0,iter_range):\n",
    "    #    print('\\t',neg_list[j], '\\t Evidence Value : ', np.sort(feature_neg)[j])\n",
    "        print(' ',j+1, f_names[neg_list[j]], '\\t Value: ', X_test[obj_idx, neg_list[j]], '\\t Evidence: ', np.sort(feature_neg)[j], '\\tCoef_: ', coef_[0, pos_list[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Copy the data X and coefficient w_i'''\n",
    "X = np.copy(X_test_scaled)\n",
    "coef_ = np.copy(clf_l2.coef_)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "'''Initialize X_ev : x_i * w_i'''\n",
    "X_ev = np.zeros((n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Get evidence'''\n",
    "'''Calculate the w_ia_i'''\n",
    "for idx in range(n_samples):\n",
    "    X_ev[idx, :] = X[idx,:] * coef_\n",
    "\n",
    "'''Generate the sets of P and N'''\n",
    "X_pos_ev = X_ev * (X_ev > 0)\n",
    "X_neg_ev = X_ev * (X_ev < 0)\n",
    "\n",
    "'''Sum each the set P and N'''\n",
    "pos_ev = np.sum(X_pos_ev, axis=1)\n",
    "neg_ev = np.sum(X_neg_ev, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most positive object with respect to the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_pos_obj_idx = np.argmax(y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Object w.r.t Probabilities\n",
      " \n",
      "Index of the object :  870\n",
      "[-0.05701343 -0.09422382 -0.3958677  ..., -0.07060258 -0.03288111\n",
      " -0.03466182]\n",
      "Class :  0\n",
      "Predict Class :  1\n",
      "----------False Positive-------------\n",
      " \n",
      "a) Total  positive evidence :  246.575760346\n",
      "b) Total negative evidence :  -207.908050888\n",
      "c) Probability distribution [ 0.  1.]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "  1 hateful \t Value:  22.3383079037 \t Evidence:  6.06728521574 \tCoef_:  0.271608988555\n",
      "  2 cultures \t Value:  22.7998538007 \t Evidence:  5.72497975419 \tCoef_:  0.251097213352\n",
      "  3 counterparts \t Value:  24.9799919936 \t Evidence:  4.46719718252 \tCoef_:  0.17883100938\n",
      "  4 commendable \t Value:  23.2911627204 \t Evidence:  4.18851717856 \tCoef_:  0.179832893224\n",
      "  5 primary \t Value:  15.8595476772 \t Evidence:  3.77227673278 \tCoef_:  0.237855253476\n",
      "  6 devastated \t Value:  28.380479434 \t Evidence:  3.47796091513 \tCoef_:  0.122547644878\n",
      "  7 perfectly \t Value:  6.42659975673 \t Evidence:  3.32382720635 \tCoef_:  0.51719841474\n",
      "  8 comparisons \t Value:  19.8953611886 \t Evidence:  3.28147912256 \tCoef_:  0.164936896166\n",
      "  9 confronts \t Value:  25.9745197391 \t Evidence:  3.27451895017 \tCoef_:  0.126066583061\n",
      "  10 sensitivity \t Value:  24.6729831508 \t Evidence:  3.22070924256 \tCoef_:  0.130535866817\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "  1 endured \t Value:  28.380479434 \t Evidence:  -6.16636223664 \tCoef_:  0.130535866817\n",
      "  2 thereafter \t Value:  25.6299578002 \t Evidence:  -4.43767549736 \tCoef_:  0.130535866817\n",
      "  3 uncertain \t Value:  24.9799919936 \t Evidence:  -4.11329289584 \tCoef_:  0.130535866817\n",
      "  4 regarded \t Value:  18.6070476493 \t Evidence:  -4.02862124628 \tCoef_:  0.130535866817\n",
      "  5 shambles \t Value:  30.4125948568 \t Evidence:  -3.75252617206 \tCoef_:  0.130535866817\n",
      "  6 sadistic \t Value:  15.7011067277 \t Evidence:  -3.74295456338 \tCoef_:  0.130535866817\n",
      "  7 fails \t Value:  6.59423883137 \t Evidence:  -3.69151687545 \tCoef_:  0.130535866817\n",
      "  8 depicting \t Value:  17.5397256338 \t Evidence:  -3.43700505687 \tCoef_:  0.130535866817\n",
      "  9 digs \t Value:  29.343976648 \t Evidence:  -3.37429535436 \tCoef_:  0.130535866817\n",
      "  10 sophistication \t Value:  25.2987280515 \t Evidence:  -3.06725559171 \tCoef_:  0.130535866817\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Most Positive Object w.r.t Probabilities', \n",
    "                      most_pos_obj_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most negative object with respect to the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_neg_obj_idx = np.argmin(y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Negative Object w.r.t Probabilities\n",
      " \n",
      "Index of the object :  10505\n",
      "[-0.05701343 -0.09422382 -0.3958677  ..., -0.07060258 -0.03288111\n",
      " -0.03466182]\n",
      "Class :  0\n",
      "Predict Class :  0\n",
      "a) Total  positive evidence :  127.850456971\n",
      "b) Total negative evidence :  -273.93573712\n",
      "c) Probability distribution [  1.00000000e+00   2.50062493e-64]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "  1 nerdy \t Value:  27.0978618649 \t Evidence:  4.44843873911 \tCoef_:  0.164161983012\n",
      "  2 servant \t Value:  18.7380008554 \t Evidence:  4.25232782324 \tCoef_:  0.226936045956\n",
      "  3 washed \t Value:  20.0555679663 \t Evidence:  3.46922698246 \tCoef_:  0.172980739727\n",
      "  4 rocks \t Value:  14.0503629282 \t Evidence:  3.46200150079 \tCoef_:  0.246399435979\n",
      "  5 tortured \t Value:  15.0423644176 \t Evidence:  3.0355835299 \tCoef_:  0.201802286238\n",
      "  6 lucky \t Value:  9.92983351449 \t Evidence:  2.95922316126 \tCoef_:  0.298013371215\n",
      "  7 magic \t Value:  7.89247358187 \t Evidence:  2.09249006288 \tCoef_:  0.265124747163\n",
      "  8 washing \t Value:  29.343976648 \t Evidence:  2.0703123156 \tCoef_:  0.070553229388\n",
      "  9 captured \t Value:  10.0935949213 \t Evidence:  2.00996570386 \tCoef_:  0.199132788618\n",
      "  10 masterpiece \t Value:  6.49445093282 \t Evidence:  1.70252750552 \tCoef_:  0.262151107635\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "  1 talentless \t Value:  29.343976648 \t Evidence:  -7.60517421313 \tCoef_:  0.262151107635\n",
      "  2 unwatchable \t Value:  15.6236371955 \t Evidence:  -6.40124879392 \tCoef_:  0.262151107635\n",
      "  3 pirates \t Value:  23.8155793165 \t Evidence:  -5.05894785935 \tCoef_:  0.262151107635\n",
      "  4 tripe \t Value:  18.1093171716 \t Evidence:  -4.87135733574 \tCoef_:  0.262151107635\n",
      "  5 paint \t Value:  12.0145260918 \t Evidence:  -4.68076011301 \tCoef_:  0.262151107635\n",
      "  6 yep \t Value:  21.6955781948 \t Evidence:  -4.57739570886 \tCoef_:  0.262151107635\n",
      "  7 waste \t Value:  4.2628369827 \t Evidence:  -4.54006573292 \tCoef_:  0.262151107635\n",
      "  8 conveniently \t Value:  20.9188071177 \t Evidence:  -4.35318446792 \tCoef_:  0.262151107635\n",
      "  9 lousy \t Value:  11.0798184208 \t Evidence:  -4.30813540326 \tCoef_:  0.262151107635\n",
      "  10 worst \t Value:  3.16513343071 \t Evidence:  -4.21434987045 \tCoef_:  0.262151107635\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Most Negative Object w.r.t Probabilities', \n",
    "                      most_neg_obj_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The object that has the largest positive evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_pos_ev_idx = np.argmax(pos_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object with Largest Positive Evidence\n",
      " \n",
      "Index of the object :  10505\n",
      "[-0.05701343 -0.09422382 -0.3958677  ..., -0.07060258 -0.03288111\n",
      " -0.03466182]\n",
      "Class :  0\n",
      "Predict Class :  0\n",
      "a) Total  positive evidence :  127.850456971\n",
      "b) Total negative evidence :  -273.93573712\n",
      "c) Probability distribution [  1.00000000e+00   2.50062493e-64]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "  1 nerdy \t Value:  27.0978618649 \t Evidence:  4.44843873911 \tCoef_:  0.164161983012\n",
      "  2 servant \t Value:  18.7380008554 \t Evidence:  4.25232782324 \tCoef_:  0.226936045956\n",
      "  3 washed \t Value:  20.0555679663 \t Evidence:  3.46922698246 \tCoef_:  0.172980739727\n",
      "  4 rocks \t Value:  14.0503629282 \t Evidence:  3.46200150079 \tCoef_:  0.246399435979\n",
      "  5 tortured \t Value:  15.0423644176 \t Evidence:  3.0355835299 \tCoef_:  0.201802286238\n",
      "  6 lucky \t Value:  9.92983351449 \t Evidence:  2.95922316126 \tCoef_:  0.298013371215\n",
      "  7 magic \t Value:  7.89247358187 \t Evidence:  2.09249006288 \tCoef_:  0.265124747163\n",
      "  8 washing \t Value:  29.343976648 \t Evidence:  2.0703123156 \tCoef_:  0.070553229388\n",
      "  9 captured \t Value:  10.0935949213 \t Evidence:  2.00996570386 \tCoef_:  0.199132788618\n",
      "  10 masterpiece \t Value:  6.49445093282 \t Evidence:  1.70252750552 \tCoef_:  0.262151107635\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "  1 talentless \t Value:  29.343976648 \t Evidence:  -7.60517421313 \tCoef_:  0.262151107635\n",
      "  2 unwatchable \t Value:  15.6236371955 \t Evidence:  -6.40124879392 \tCoef_:  0.262151107635\n",
      "  3 pirates \t Value:  23.8155793165 \t Evidence:  -5.05894785935 \tCoef_:  0.262151107635\n",
      "  4 tripe \t Value:  18.1093171716 \t Evidence:  -4.87135733574 \tCoef_:  0.262151107635\n",
      "  5 paint \t Value:  12.0145260918 \t Evidence:  -4.68076011301 \tCoef_:  0.262151107635\n",
      "  6 yep \t Value:  21.6955781948 \t Evidence:  -4.57739570886 \tCoef_:  0.262151107635\n",
      "  7 waste \t Value:  4.2628369827 \t Evidence:  -4.54006573292 \tCoef_:  0.262151107635\n",
      "  8 conveniently \t Value:  20.9188071177 \t Evidence:  -4.35318446792 \tCoef_:  0.262151107635\n",
      "  9 lousy \t Value:  11.0798184208 \t Evidence:  -4.30813540326 \tCoef_:  0.262151107635\n",
      "  10 worst \t Value:  3.16513343071 \t Evidence:  -4.21434987045 \tCoef_:  0.262151107635\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Object with Largest Positive Evidence', \n",
    "                      most_neg_obj_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The object that has the largest negative evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_neg_ev_idx = np.argmin(neg_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object with Largest Negative Evidence\n",
      " \n",
      "Index of the object :  18112\n",
      "[-0.05701343 -0.09422382 -0.3958677  ..., -0.07060258 -0.03288111\n",
      " -0.03466182]\n",
      "Class :  1\n",
      "Predict Class :  0\n",
      "----------False Positive-------------\n",
      " \n",
      "a) Total  positive evidence :  382.427437176\n",
      "b) Total negative evidence :  -427.889603233\n",
      "c) Probability distribution [  1.00000000e+00   1.25346369e-20]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "  1 conquest \t Value:  27.9329554469 \t Evidence:  9.18373672488 \tCoef_:  0.328777838862\n",
      "  2 photograph \t Value:  25.9745197391 \t Evidence:  8.42567162368 \tCoef_:  0.324382191021\n",
      "  3 disappoint \t Value:  15.9405784342 \t Evidence:  5.69362841332 \tCoef_:  0.357178281631\n",
      "  4 stadium \t Value:  27.5059222273 \t Evidence:  5.47382474842 \tCoef_:  0.199005316135\n",
      "  5 wherein \t Value:  30.4125948568 \t Evidence:  5.35278625715 \tCoef_:  0.176005575399\n",
      "  6 supports \t Value:  26.3333333333 \t Evidence:  4.85451549412 \tCoef_:  0.18434868965\n",
      "  7 abandoned \t Value:  11.8424211145 \t Evidence:  4.78346288828 \tCoef_:  0.403926092649\n",
      "  8 responds \t Value:  25.2987280515 \t Evidence:  4.67153030797 \tCoef_:  0.18465475017\n",
      "  9 admirer \t Value:  25.9745197391 \t Evidence:  4.63893444741 \tCoef_:  0.178595581131\n",
      "  10 neglect \t Value:  26.7074093518 \t Evidence:  3.65841169289 \tCoef_:  0.136981151736\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "  1 unimaginative \t Value:  20.3879049112 \t Evidence:  -6.08919508175 \tCoef_:  0.136981151736\n",
      "  2 outrage \t Value:  28.380479434 \t Evidence:  -5.79624237462 \tCoef_:  0.136981151736\n",
      "  3 pirates \t Value:  23.8155793165 \t Evidence:  -5.05894785935 \tCoef_:  0.136981151736\n",
      "  4 distress \t Value:  22.5655507718 \t Evidence:  -4.53311288695 \tCoef_:  0.136981151736\n",
      "  5 thereafter \t Value:  25.6299578002 \t Evidence:  -4.43767549736 \tCoef_:  0.136981151736\n",
      "  6 worst \t Value:  3.16513343071 \t Evidence:  -4.21434987045 \tCoef_:  0.136981151736\n",
      "  7 poorly \t Value:  6.22459862007 \t Evidence:  -4.19296982057 \tCoef_:  0.136981151736\n",
      "  8 generous \t Value:  14.3990740443 \t Evidence:  -4.12691831792 \tCoef_:  0.136981151736\n",
      "  9 literature \t Value:  17.4320982323 \t Evidence:  -4.11435640278 \tCoef_:  0.136981151736\n",
      "  10 insult \t Value:  10.9175229651 \t Evidence:  -4.02376445069 \tCoef_:  0.136981151736\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('Object with Largest Negative Evidence', \n",
    "                      most_neg_ev_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most uncertain object with respect to the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uncertain_idx = np.argmin(np.square(y_pred_proba[:,1]-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most uncertain object\n",
      " \n",
      "Index of the object :  20173\n",
      "[-0.05701343 -0.09422382 -0.3958677  ..., -0.07060258 -0.03288111\n",
      " -0.03466182]\n",
      "Class :  1\n",
      "Predict Class :  0\n",
      "----------False Positive-------------\n",
      " \n",
      "a) Total  positive evidence :  93.2688955077\n",
      "b) Total negative evidence :  -92.9057482248\n",
      "c) Probability distribution [ 0.50011946  0.49988054]\n",
      "d) Top 10 features values that contribute most to the positive evidence\n",
      "  1 accompanying \t Value:  23.5490032816 \t Evidence:  4.52513525647 \tCoef_:  0.192158249857\n",
      "  2 opposite \t Value:  9.85084847545 \t Evidence:  2.24566161822 \tCoef_:  0.227966314152\n",
      "  3 traditional \t Value:  10.3776515732 \t Evidence:  2.17389692394 \tCoef_:  0.209478696468\n",
      "  4 insane \t Value:  11.0249271929 \t Evidence:  1.75189438735 \tCoef_:  0.158903034614\n",
      "  5 capable \t Value:  10.564687057 \t Evidence:  1.6711730641 \tCoef_:  0.158184814665\n",
      "  6 great \t Value:  1.72048899145 \t Evidence:  1.62984964849 \tCoef_:  0.947317685021\n",
      "  7 shows \t Value:  3.47200666578 \t Evidence:  1.60267709157 \tCoef_:  0.461599658597\n",
      "  8 vivid \t Value:  15.4720591837 \t Evidence:  1.50512809948 \tCoef_:  0.0972803995643\n",
      "  9 gritty \t Value:  11.6452574835 \t Evidence:  1.46040652437 \tCoef_:  0.1254078346\n",
      "  10 poignant \t Value:  12.6196834452 \t Evidence:  1.43884267166 \tCoef_:  0.114015749913\n",
      "e) Top 10 features values that contribute most to the negative evidence\n",
      "  1 returned \t Value:  14.7749194791 \t Evidence:  -3.28617707224 \tCoef_:  0.114015749913\n",
      "  2 worse \t Value:  4.31903786164 \t Evidence:  -3.05093791806 \tCoef_:  0.114015749913\n",
      "  3 commander \t Value:  20.7372727898 \t Evidence:  -2.95636207647 \tCoef_:  0.114015749913\n",
      "  4 surrender \t Value:  27.0978618649 \t Evidence:  -2.8068328266 \tCoef_:  0.114015749913\n",
      "  5 horrible \t Value:  4.89134177164 \t Evidence:  -2.5354374114 \tCoef_:  0.114015749913\n",
      "  6 accompany \t Value:  29.343976648 \t Evidence:  -2.51190359732 \tCoef_:  0.114015749913\n",
      "  7 blow \t Value:  11.4575360473 \t Evidence:  -2.13730333079 \tCoef_:  0.114015749913\n",
      "  8 soldiers \t Value:  9.19871938675 \t Evidence:  -1.98222177869 \tCoef_:  0.114015749913\n",
      "  9 glory \t Value:  13.5212251446 \t Evidence:  -1.84103168032 \tCoef_:  0.114015749913\n",
      "  10 problem \t Value:  4.37550503589 \t Evidence:  -1.72952113802 \tCoef_:  0.114015749913\n"
     ]
    }
   ],
   "source": [
    "print_object_evidence('The most uncertain object', \n",
    "                      uncertain_idx, X_test_scaled, y_pred, y_pred_proba, X_ev, pos_ev, neg_ev, f_names, iter_range=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
